# toxic-comment-moderation

After EDA it was observed that amount of comments which could are toxic in general is 22%. By splitting all toxic comments into separate classes as:

'toxic' 
'severe_toxic'
'obscene' 
'threat'
'insult' 
'identity_hate', 

we get more concrete information about how specifically each comment is toxic (if it is).

Most of the comments have significant disbalance in terms of its length, belonging to a specific class (there is only 5% of threat comments for example).
This information is useful for future analysis and input limitations in our model.



